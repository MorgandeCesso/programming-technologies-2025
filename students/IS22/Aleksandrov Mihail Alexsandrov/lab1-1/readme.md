# Отчёт. Лабораторная работа №1. Введение в DL.
## Задание
Необходимо познакомиться с фреймворком машинного обучения PyTorch и выполнить три задания:
 - Регрессия по теореме универсальной аппроксимации, ручное дифференцирование
 - Бинарная классификация с помощью автодиффиренцирования PyTorch
 - Обучить полносвязную нейронную сеть классификации 3 классов изображений из набора данных CIFAR100 по варианту из примера и затем повысить точность на тестовой выборке.

# Ход работы 
## Задание 1 

В первом задании нужно было реализовать задачу регрессии с использованием нейронной сети с одним скрытым слоем и активационной функцией ReLU. Нужно показать по теореме универсальной аппроксимации, что простая сеть способна приближать произвольные непрерывные функции. Так как в задаче нужно реализовать ручное дифференцирование, значит обучение сети должно выполняться вручную, без использования встроенных средств автоматического дифференцирования.

Для решения задания в начале реализуем генерацию выборки. Для этого создаём массив входных данных в диапазоне от −0.5 до 0.5, при этом каждое значение повторяется пять раз. Далее вычисляем сигмоидальную функцию для каждого значения массива X. После этого формируем случайный шум с помощью добавления к каждому истинному значению функции случайный гауссовский шум с нулевым средним.

![alt text](photo/1.png)

_Рисунок 1: График зашумлённых данных и истинных значений сигмоидальной функции_

Далее создаём простую нейронную сеть с одним скрытым слоем. Для этого переводим массивы X и yn в формат PyTorch, чтобы данные имели корректную форму в виде количества объектов и размерности признаков. Данные такого вида мы сможем использовать в матричных операциях сети. После этого задаём размер скрытого слоя 64, определяя количество нейронов. Далее создаём случайные веса для скрытого и выходного слоя, а также создаём нулевые смещения, которые позволяют нейронам корректно реагировать на входные данные и обучаться во время градиентного спуска.

```python
tensor_X = torch.Tensor(X.reshape(-1, 1))
tensor_y = torch.Tensor(yn.reshape(-1, 1))
HIDDEN_SIZE = 64
weights_1 = (torch.rand(1, HIDDEN_SIZE)-.5)/10
bias_1 = torch.zeros(HIDDEN_SIZE)
weights_2 = (torch.rand(HIDDEN_SIZE, 1)-.5)/10
bias_2 = torch.zeros(1)
```

Теперь перейдём к обучению нейронной сети задачи регрессии. Создаём активационную функцию ReLU, которая на выходе заменяет все отрицательные значения на 0, а положительные оставляет без изменений. Далее реализуем прямой проход нейросети: входное значение сначала умножается на веса первого слоя и к нему добавляется смещение скрытого слоя, затем результат проходит через ранее созданную функцию активации ReLU, после чего выход скрытого слоя взвешивается весами второго слоя, суммируется и к нему добавляется смещение выходного нейрона, формируя итоговое предсказание сети. Создаём функцию loss, вычисляя сумму квадратов разностей между предсказанными значениями и истинными. Данная функция показывает насколько сильно сеть ошибается на каждой точке.

```python
relu = lambda x: torch.maximum(x, torch.Tensor([0]))
forward = lambda x: (weights_2.t()*relu((weights_1*x) + bias_1)
                      ).sum(axis=-1,keepdims=True) + bias_2
loss = lambda y, y_: ((y-y_)**2).sum(axis=-1)
```

Далее создаём функцию backward, которая вычисляет градиенты функции потерь по всем весам и смещениям сети, учитывая ReLU в скрытом слое, чтобы затем обновить параметры нейросети с помощью градиентного спуска. По сути реализует обратный проход по нейронной сети.

```python
def backward(X, y, y_pred):
    dL = 2*(y_pred-y)
    Ax = (weights_1*X) + bias_1
    A = relu(Ax)
    dW2 = torch.mm(A.t(), dL)
    db2 = dL.sum(axis=0)
    dA = torch.mm(dL, weights_2.t())
    dA[Ax<=0] = 0
    dW = torch.mm(X.t(), dA)
    db = dA.sum(axis=0)

    return dW, db, dW2, db2
```

После создаём функцию optimize, которая реализует сам градиентный спуск. Обновляются веса и смещения с учётом вычисленных градиентов, чтобы уменьшить ошибку сети.

```python
def optimize(params, grads, lr=0.001):
    W1, b1, W2, b2 = params
    W1 -= lr*grads[0]
    W2 -= lr*grads[2]
    b1 -= lr*grads[1]
    b2 -= lr*grads[3]

    return W1, b1, W2, b2
```

Теперь создаём основной цикл обучения нейросети на 50 тыс эпох. Для этого с помощью ранее созданных функций вычисляем текущее предсказание сети, текущую ошибку сети, градиенты функции потерь. И применяем градиентный спуск для списка весов и смещений.

```python
for i in range(50000):
  output = forward(tensor_X)
  cur_loss = loss(output, tensor_y)
  grads = backward(tensor_X, tensor_y, output)
  params = [weights_1, bias_1, weights_2, bias_2]
  weights_1, bias_1, weights_2, bias_2 = optimize(params, grads, 1e-4)
```

Графическое отображение предсказаниями сети:

![alt text](photo/2.png)

_Рисунок 2: График с истинной функцией и предсказаниями сети каждые 10 тысяч итераций_

На рисунке представлен график зависимости значения вода от выхода. Предположительно функция нам не известна, есть лишь коллекция значений типа "вход-выход". Обучение сети для получения аппроксимации функции заключается в нахождении взаимосвязи этих параметров для продолжения построения графика.

Размерность скрытого слоя была выствлена в 1024, что дало уменьшению дельты между значениеми графиков различного количества эпох.

## Задание 2

Бинарная классификация заключается в определнии различия между двумя объектами.
Для примера решения задач использовалась задача XOR.

![alt text](photo/3.png)

_Рисунок 3: Задача XOR_

Однослойный персептрон не способен решить данную задачу, ввиду своей линейности, в отличии от многослойного персептрона.

Процесс обучения заключается в минимизации функции потерь. Данная задача была выполнена, что подтверждает следующее изображение.

![alt text](photo/4.png)

_Рисунок 4: График потерь от итераций_

Результат классификации:

![alt text](photo/5.png)

_Рисунок 5: XOR_

## Задание 3

Финальным задание была классификация изображений из выборки CIFAR100. Классификация изображений CIFAR-100 — это задача компьютерного зрения, в которой модель должна определить, к какому из 100 классов относится изображение.

Пример изображения из выборки CIFAR100:

![alt text](photo/6.png)

_Рисунок 6: Изображение из CIFAR100_

Результат обучения классификации образов:

![alt text](photo/7.png)

_Рисунок 7: Результат обучения_

Сравнение результатов:

![alt text](photo/8.png)

_Рисунок 8: Сравнение результатов_

# Вывод
В ходе выполнения лабораторной работы была изучена библиотека PyTorch, предназначенная для создания и обучения нейронных сетей. Кроме того, исследован датасет CIFAR100, который, в отличие от MNIST, состоит из полноцветных (RGB) изображений реальных объектов, что усложняет задачу классификации.
